{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: pre-processing pipeline\n",
    "we have created a pre-processing pipeline suitable for large files from the data processing functions of task 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zeyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Using 'wordpunct_tokenize' to split text on whitespace and punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "def tokenize_data(data):    \n",
    "    tokenized_data = word_tokenize(data)\n",
    "    tok_punct_data = [word for word in tokenized_data if any(char in string.ascii_letters for char in word)]\n",
    "    return(tok_punct_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using NLTK's in-built collection of stopwords \n",
    "from nltk.corpus import stopwords\n",
    "import os \n",
    "# Stopwords from nltk\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "# collecting more stopwords from website: http://members.unine.ch/jacques.savoy/clef/ given in lecture\n",
    "stop_words_extra_path = os.path.join(os.getcwd(), \"../stopwords_extra.txt\")\n",
    "stop_words_extra = set(open(stop_words_extra_path, \"r\").read().split(\"\\n\"))\n",
    "stop_words = stop_words_nltk | stop_words_extra \n",
    "\n",
    "# removing stopwords\n",
    "def stopwords_data(data_list):\n",
    "    #stopwords are removed\n",
    "    stopword_data = [word for word in data_list if word not in stop_words and sum(1 for char in word if char in string.ascii_letters) > 1]\n",
    "    return stopword_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# initialize the stemmer\n",
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "placeholders = ['NUM', 'URL', 'EMAIL', 'DATE']\n",
    "# stemming\n",
    "def stem_data(data):\n",
    "    stemmed_data =  [stemmer.stem(word) if word not in placeholders else word for word in data]\n",
    "    return stemmed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex_clean(text):\n",
    "    # lowercase\n",
    "    pattern_lowercase = re.compile('[A-Z]')\n",
    "    cleaned_text = re.sub(pattern_lowercase, lambda x: x.group(0).lower(), text)\n",
    "    # whitespace\n",
    "    pattern_whitespace = re.compile(' {2,}')\n",
    "    cleaned_text = re.sub(pattern_whitespace, \" \", cleaned_text)\n",
    "    # newline\n",
    "    pattern_newline = re.compile('\\n+')\n",
    "    cleaned_text = re.sub(pattern_newline, \"\\n\", cleaned_text)\n",
    "    # tab\n",
    "    pattern_tab = re.compile('\\t+')\n",
    "    cleaned_text = re.sub(pattern_tab, \"\\t\", cleaned_text)\n",
    "    # emails\n",
    "    pattern_email = re.compile('''([^,|\\\"|\\|| |\\t|\\n|'|\\]|\\[]*@[^,|\\\"|\\|| |\\t|\\n|'|\\]|\\[]*\\.(com|org|edu|uk|net|gov))''')\n",
    "    cleaned_text = re.sub(pattern_email, \"<EMAIL>\", cleaned_text)\n",
    "    # URL's\n",
    "    pattern_URL1 = re.compile('''([^,|\\\"|\\|| |\\t|\\n|'|\\]|\\[]*\\.(com|org|edu|uk|net|gov)[^,|\\\"|\\|| |\\t|\\n|'|\\]|\\[]*)''')         # top-level domains\n",
    "    pattern_URL2 = re.compile('''https?:\\/\\/[^,|\\\"|\\|| |\\t|\\n|'|\\]|\\[]*''')                                                           # http(s) \n",
    "    cleaned_text = re.sub(pattern_URL1, \"<URL>\", cleaned_text)\n",
    "    cleaned_text = re.sub(pattern_URL2, \"<URL>\", cleaned_text)\n",
    "    # dates \n",
    "    pattern_dates = re.compile('''(((0[1-9]|[1-2]\\d|3[0-1])(\\-|\\/|\\.|\\,| ){1,2}(0[1-9]|1[1-2]|[a-z]{3,9})(\\-|\\/|\\.|\\,| ){1,2}(\\d{2,}))|((0[1-9]|1[1-2]|[a-z]{3,9})(\\-|\\/|\\.|\\,| )(0[1-9]|[1-2]\\d|3[0-1])(\\-|\\/|\\.|\\,| ){1,2}(\\d{2,}))|((\\d{2,}))(\\-|\\/|\\.|\\,| )(0[1-9]|1[1-2]|[a-z]{3,9})(\\-|\\/|\\.|\\,| ){1,2}(0[1-9]|[1-2]\\d|3[0-1])|((jan|january|feb|febuary|apr|april|may|jun|june|aug|august|sep|september|oct|october|nov|november|dec|december)(\\-|\\/|\\.|\\,| ){1,2}(0[1-9]|[1-2]\\d|3[0-1])))''')\n",
    "    cleaned_text = re.sub(pattern_dates, '<DATE>', cleaned_text)\n",
    "    # numbers\n",
    "    pattern_numbers = re.compile('(\\d,\\d|\\d\\.\\d|\\d)+')\n",
    "    pattern_numbers_2 = re.compile('((\\d:\\d|\\d,\\d|\\d\\.\\d|\\d)+)')\n",
    "    cleaned_text = re.sub(pattern_numbers_2, '<NUM>', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function applies the preprocessing pipeline to a fake news .csv file, creating a preprocessed version of that .csv file, and returning vocabulary information for data exploration.\n",
    "\n",
    "It processes chunks of 100,000 rows at a time. This is to prevent the 3.4 Gb file 995,000_rows.csv from crashing the program. takes a while to process the entire file; around 6 hours.\n",
    "\n",
    "The pipeline removes problematic rows (no content, content in russian, no type, wrong type). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "def data_preprocessing(filepath):\n",
    "\n",
    "    directory, filename = os.path.split(filepath)\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    preprocessed_file_path = os.path.join(directory,f\"{base}_preprocessed{ext}\")\n",
    "    statistics_file_path = os.path.join(directory,f\"{base}_statistics.pickle\")\n",
    "    print(f\"new cleaned dataset:\", preprocessed_file_path)\n",
    "    print(f\"new statistics folder:\", statistics_file_path)\n",
    "    cleaned_vocab = Counter()\n",
    "    processed_fake_vocab = Counter()\n",
    "    processed_real_vocab = Counter()\n",
    "\n",
    "    erronious_content = 0\n",
    "    erronious_type = 0\n",
    "\n",
    "    chunk_size = 100000\n",
    "    with open(preprocessed_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for chunk in pd.read_csv(filepath, chunksize=chunk_size,low_memory=False):\n",
    "            #drop data with erronious/empty columns.\n",
    "            erronious_content += chunk['content'].isnull().sum()\n",
    "            chunk.dropna(subset=['content'],inplace=True)\n",
    "\n",
    "            valid = ['fake','satire','bias','conspiracy','state','junksci','hate','clickbait','unreliable','political','reliable']\n",
    "            erronious_type += chunk['type'].isnull().sum() + (~(chunk['type'].isin(valid))).sum()\n",
    "            chunk.dropna(subset=['type'],inplace=True)\n",
    "            chunk.drop(chunk[~chunk['type'].isin(valid)].index, inplace=True)\n",
    "\n",
    "            #process data and gather vocabulary info.\n",
    "            chunk['content']=chunk['content'].apply(regex_clean)\n",
    "            chunk['content']=chunk['content'].apply(tokenize_data)\n",
    "            cleaned_vocab.update(token for token_list in chunk['content'] for token in token_list)\n",
    "            chunk['content']=chunk['content'].apply(stopwords_data)\n",
    "            chunk['content']=chunk['content'].apply(stem_data)\n",
    "            processed_fake_vocab.update(token for _, row in chunk.iterrows() if row['type'] not in ['reliable','political'] for token in row['content'])\n",
    "            processed_real_vocab.update(token for _, row in chunk.iterrows() if row['type'] in ['reliable','political'] for token in row['content'])\n",
    "\n",
    "            #drop data with empty content after processing\n",
    "            erronious_content += (chunk['content'].apply(len) == 0).sum()\n",
    "            chunk.drop(chunk[chunk['content'].apply(len) == 0].index, inplace = True)\n",
    "            \n",
    "            #return back into a string, words/tokens seperated by one space\n",
    "            chunk['content'] = chunk['content'].apply(lambda row: ' '.join(row))\n",
    "            #write to output file\n",
    "            chunk.to_csv(preprocessed_file_path, header=False, index=False, mode='a')\n",
    "    \n",
    "    #write to statistics folder:\n",
    "    with open(statistics_file_path, 'wb') as f:\n",
    "        pickle.dump(cleaned_vocab,f)\n",
    "        pickle.dump(processed_fake_vocab,f)\n",
    "        pickle.dump(processed_real_vocab,f)\n",
    "        pickle.dump(erronious_type,f)\n",
    "        pickle.dump(erronious_content,f)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying the pipeline to 995,000_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new cleaned dataset: ../995,000_rows_preprocessed.csv\n",
      "new statistics folder: ../995,000_rows_statistics.pickle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "file = \"../995,000_rows.csv\"\n",
    "data_preprocessing(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying the preprocessing pipeline to the BBC articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the bbc articles were stored in a .txt, so it is first converted to .csv, then preprocessed. It contains fewer columns than other articles, but it has everything our models need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "BBC_df = pd.DataFrame(columns=[\"domain\",\"type\",\"content\",\"title\"])\n",
    "with open('../BBC_articles.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            dictionary = eval(line)\n",
    "            row = {\"domain\":\"BBC.com\",\"type\":\"reliable\",\"content\":dictionary[\"TEXT\"],\"title\":dictionary[\"HEADLINE\"]}\n",
    "            BBC_df.loc[len(BBC_df)] = row\n",
    "BBC_df.to_csv('../BBC_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new cleaned dataset: ../BBC_articles_preprocessed.csv\n",
      "new statistics folder: ../BBC_articles_statistics.pickle\n"
     ]
    }
   ],
   "source": [
    "data_preprocessing('../BBC_articles.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
