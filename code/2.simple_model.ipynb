{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple model\n",
    "\n",
    "Running all models at once might crash the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Choice of label groups\n",
    "\n",
    "political and reliable has been chosen as \"real news\", while the rest as \"fake news\". some categories like unknown or unreliable might include legitimate articles which were put in their categories simply due to the lack of validation and confirmation. This might make it difficult for the model to correctly categorize these articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "data = np.load('../Simple_995.npz', allow_pickle=True)\n",
    "data_BBC = np.load('../Simple_995_BBC.npz', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 1: Simple model baselines\n",
    "\n",
    "There are two baseline models, one with one-hot encoding, the other a Bag of Words encoding. both use logistic regression, as it is the simplest classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression with one-hot encoding:\n",
    "\n",
    "This is the simplest model which made sence that we could think of. It uses one-hot representations of the articles' vocabularies to classify them. However, this representation does not take frequency into consideration, making it ineffective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5818633540372671\n",
      "Precision: 0.5397275899998643\n",
      "Recall: 0.9648249745108511\n",
      "F1 score: 0.6922226189335818\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "#load features\n",
    "X_train=data['X_train_content_ONEHOT'].item()\n",
    "X_val=data['X_val_content_ONEHOT'].item()\n",
    "Y_train = data['Y_train'].ravel()\n",
    "Y_val = data['Y_val'].ravel()\n",
    "\n",
    "#fit,predict and evaluate\n",
    "model = LogisticRegression(tol=1e-3, max_iter=1000,penalty = None)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\",accuracy_score(Y_val,Y_pred))\n",
    "print(\"Precision:\",precision_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"Recall:\",recall_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"F1 score:\",f1_score(Y_val,Y_pred,average=\"binary\"))\n",
    "    \n",
    "\n",
    "with open(\"../simple_model_ONEHOT.pkl\",'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "del model, X_train, X_val, Y_train, Y_val, Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with Bag of words encoding:\n",
    "\n",
    "This is the simple model baseline takes frequency into consideration, and yields considerably better results.\n",
    "\n",
    "This is the simple model used from here on out, as it is simple, but not too much so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8691156462585033\n",
      "Precision: 0.8838441743738694\n",
      "Recall: 0.8421129290673399\n",
      "F1 score: 0.8624740499484107\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "#load features\n",
    "X_train=data['X_train_content_BOW'].item()\n",
    "X_val=data['X_val_content_BOW'].item()\n",
    "Y_train = data['Y_train'].ravel()\n",
    "Y_val = data['Y_val'].ravel()\n",
    "\n",
    "#fit,predict and evaluate\n",
    "model = LogisticRegression(tol=1e-3, max_iter=1000,penalty = None)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\",accuracy_score(Y_val,Y_pred))\n",
    "print(\"Precision:\",precision_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"Recall:\",recall_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"F1 score:\",f1_score(Y_val,Y_pred,average=\"binary\"))\n",
    "    \n",
    "del model, X_train, X_val, Y_train, Y_val, Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Which column features might be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration, it is clear that some domains produce much more real/fake news than others. This would make domain a good indicator of fake/real news.\n",
    "\n",
    "Furthermore, the title of the article would also be helpful; \"fake\" articles, especially, especially ones like \"hate\", \"extreme bias\", \"clickbait\", etc. will have distinct titles that are provocative, misleading or exaggerated. This feature would therefore help distinguish fake/real news.\n",
    "\n",
    "The rest of the columns have too many empty cells to be used as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain:\n",
    "\n",
    "Adding domain features to the simple model, we can see that the model gives a almost perfect prediction, which is surprising.\n",
    "\n",
    "Training the simple model with only domian features, we get a perfect prediction, indicating that the domain of an article is directly tied to it being real or fake.\n",
    "\n",
    "Because of this, it would not make sense to include domain in the model, at it effectively serves as an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using Content and Domain\n",
      "Accuracy: 0.9852351375332742\n",
      "Precision: 0.9872176416060887\n",
      "Recall: 0.9824246249453804\n",
      "F1 score: 0.9848153015038692\n",
      "Model using Domain only\n",
      "Accuracy: 0.9997633836143153\n",
      "Precision: 1.0\n",
      "Recall: 0.9995144924018061\n",
      "F1 score: 0.9997571872571872\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "#load features\n",
    "X_train_content=data['X_train_content_BOW'].item()\n",
    "X_val_content=data['X_val_content_BOW'].item()\n",
    "X_train_domain = data['X_train_domain'].item()\n",
    "X_val_domain = data['X_val_domain'].item()\n",
    "\n",
    "Y_train = data['Y_train'].ravel()\n",
    "Y_val = data['Y_val'].ravel()\n",
    "\n",
    "#combine content and domain features\n",
    "X_train = hstack([X_train_content,X_train_domain])\n",
    "X_val = hstack([X_val_content, X_val_domain])\n",
    "\n",
    "#train predict and evaluate with content and domain\n",
    "model = LogisticRegression(tol=1e-3, max_iter=1000,penalty = None)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_val)\n",
    "print(\"Model using Content and Domain\")\n",
    "print(\"Accuracy:\",accuracy_score(Y_val,Y_pred))\n",
    "print(\"Precision:\",precision_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"Recall:\",recall_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"F1 score:\",f1_score(Y_val,Y_pred,average=\"binary\"))\n",
    "    \n",
    "#train predict and evaluate with only domain\n",
    "model1 = LogisticRegression(tol=1e-3)\n",
    "model1.fit(X_train_domain, Y_train)\n",
    "Y_pred = model1.predict(X_val_domain)\n",
    "print(\"Model using Domain only\")\n",
    "print(\"Accuracy:\",accuracy_score(Y_val,Y_pred))\n",
    "print(\"Precision:\",precision_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"Recall:\",recall_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"F1 score:\",f1_score(Y_val,Y_pred,average=\"binary\"))\n",
    "    \n",
    "\n",
    "del model,model1, X_train_content, X_val_content,X_train_domain, X_val_domain, Y_train, Y_val, Y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title:\n",
    "\n",
    "adding title features gives much better performance. This indicates, that the title of an article helps distinguish real news from fake ones. It therefore makes sence to include title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model using Content and Title\n",
      "Accuracy: 0.8976989056492162\n",
      "Precision: 0.8974283830317239\n",
      "Recall: 0.8920473855415837\n",
      "F1 score: 0.8947297938909923\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "#load features\n",
    "X_train_content=data['X_train_content_BOW'].item()\n",
    "X_val_content=data['X_val_content_BOW'].item()\n",
    "X_train_title = data['X_train_title'].item()\n",
    "X_val_title = data['X_val_title'].item()\n",
    "Y_train = data['Y_train'].ravel()\n",
    "Y_val = data['Y_val'].ravel()\n",
    "\n",
    "#combine content and domain features\n",
    "X_train = hstack([X_train_content,X_train_title])\n",
    "X_val = hstack([X_val_content, X_val_title])\n",
    "\n",
    "#train predict and evaluate with content and domain\n",
    "model = LogisticRegression(tol=1e-3, max_iter=1000,penalty = None)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_val)\n",
    "print(\"Model using Content and Title\")\n",
    "print(\"Accuracy:\",accuracy_score(Y_val,Y_pred))\n",
    "print(\"Precision:\",precision_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"Recall:\",recall_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"F1 score:\",f1_score(Y_val,Y_pred,average=\"binary\"))\n",
    "\n",
    "del model, X_train_content, X_val_content,X_train_title, X_val_title, Y_train, Y_val, Y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 3: Simple model with added BBC articles\n",
    "\n",
    "Adding the BBC articles, there isnt much difference in the result. However, since the dataset had slightly more fake articles, the BBC articles are included. They make the dataset just a bit more balanced, and more data can't hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model with BBC articles\n",
      "Accuracy: 0.8706857042129384\n",
      "Precision: 0.8869285286484329\n",
      "Recall: 0.845932308278193\n",
      "F1 score: 0.8659454718222352\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "import pickle\n",
    "\n",
    "#load features\n",
    "X_train=data_BBC['X_train'].item()\n",
    "X_val = data_BBC['X_val'].item()\n",
    "Y_train = data_BBC['Y_train'].ravel()\n",
    "Y_val = data_BBC['Y_val'].ravel()\n",
    "\n",
    "#fit,predict and evaluate\n",
    "model = LogisticRegression(tol=1e-3, max_iter=1000,penalty = None)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_val)\n",
    "print(\"simple model with BBC articles\")\n",
    "print(\"Accuracy:\",accuracy_score(Y_val,Y_pred))\n",
    "print(\"Precision:\",precision_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"Recall:\",recall_score(Y_val,Y_pred,average=\"binary\"))\n",
    "print(\"F1 score:\",f1_score(Y_val,Y_pred,average=\"binary\"))\n",
    "\n",
    "with open(\"../simple_model_BOW.pkl\",'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    \n",
    "del model, X_train, X_val, Y_train, Y_val, Y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
