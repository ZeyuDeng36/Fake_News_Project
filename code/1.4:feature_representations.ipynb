{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: splitting the dataset and feature representations\n",
    "\n",
    "Spliting datasets: dataset splitting is done with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "def split_data(X, y):\n",
    "    # train/test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=0.8,random_state=0)\n",
    "    # validation split\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.5,random_state=0)\n",
    "    return X_train, X_test, X_val, Y_train, Y_test, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature represenation: final preprocessing steps\n",
    "\n",
    "Here, the preprocessed dataset is split, and feature representations for the models are created, and stored in .npz files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature representation for simple models\n",
    "\n",
    "The following code box creates Feature representations for (part 2, task 1-2).\n",
    "\n",
    "The content column of 995,000 is encoded with both countvectorizer(bag of words) and one-hot\n",
    "\n",
    "The title column is encoded with countvectorizer(bag of words)\n",
    "\n",
    "The domain column is encoded with one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tasks 2.1-2.2\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# title, domain and conent features of 995,000_rows\n",
    "title_995=(pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[9]).fillna('')).values\n",
    "domain_995=(pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[2]).fillna('')).values\n",
    "content_995 = pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[5]).values\n",
    "Y_995 = pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[3]).values\n",
    "#transform types into bool: 1-real, 0-fake\n",
    "Y_995 = np.isin(Y_995,['reliable', 'political']).astype(int)\n",
    "\n",
    "#split features\n",
    "X_train_content, X_test_content, X_val_content, Y_train, Y_test, Y_val = split_data(content_995,Y_995)\n",
    "X_train_domain, X_test_domain, X_val_domain, _, _, _ = split_data(domain_995,Y_995)\n",
    "X_train_title, X_test_title, X_val_title, _, _, _ = split_data(title_995,Y_995)\n",
    "del title_995,domain_995,content_995\n",
    "\n",
    "#one-hot for content: these parameters exclude features with frequency of under 100.\n",
    "onehot_encoder1 = OneHotEncoder(min_frequency=100,handle_unknown='infrequent_if_exist')\n",
    "#one-hot for domain: default settings.\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "#count_vectorizer (bag of words) for content: these parameters excludes features with frequency of under 100, and the words in stop_words.\n",
    "count_vectorizer = CountVectorizer(min_df=100,lowercase= False, stop_words=['NUM', 'URL', 'EMAIL', 'DATE'])\n",
    "#count_vectorizer for title: default settings.\n",
    "count_vectorizer1 = CountVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "X_train_content_BOW = count_vectorizer.fit_transform((X_train_content.reshape(-1)))\n",
    "X_test_content_BOW = count_vectorizer.transform((X_test_content.reshape(-1)))\n",
    "X_val_content_BOW = count_vectorizer.transform((X_val_content.reshape(-1)))\n",
    "del count_vectorizer\n",
    "\n",
    "X_train_content_ONEHOT = onehot_encoder1.fit_transform((X_train_content.reshape(-1,1)))\n",
    "X_test_content_ONEHOT = onehot_encoder1.transform((X_test_content.reshape(-1,1)))\n",
    "X_val_content_ONEHOT = onehot_encoder1.transform((X_val_content.reshape(-1,1)))\n",
    "del onehot_encoder1,X_train_content,X_test_content,X_val_content\n",
    "\n",
    "X_train_title = count_vectorizer1.fit_transform((X_train_title.reshape(-1)))\n",
    "X_test_title = count_vectorizer1.transform((X_test_title.reshape(-1)))\n",
    "X_val_title = count_vectorizer1.transform((X_val_title.reshape(-1)))\n",
    "del count_vectorizer1\n",
    "\n",
    "X_train_domain = onehot_encoder.fit_transform((X_train_domain.reshape(-1, 1)))\n",
    "X_test_domain = onehot_encoder.transform((X_test_domain.reshape(-1, 1)))\n",
    "X_val_domain = onehot_encoder.transform((X_val_domain.reshape(-1, 1)))\n",
    "del onehot_encoder\n",
    "\n",
    "#scale content and title features with standardscaler. this is recommended for BOW/countVectorizer features\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_content_BOW = scaler.fit_transform(X_train_content_BOW)\n",
    "X_test_content_BOW = scaler.transform(X_test_content_BOW)\n",
    "X_val_content_BOW = scaler.transform(X_val_content_BOW)\n",
    "del scaler\n",
    "\n",
    "scaler1 = StandardScaler(with_mean=False)\n",
    "X_train_title = scaler1.fit_transform(X_train_title)\n",
    "X_test_title = scaler1.transform(X_test_title)\n",
    "X_val_title = scaler1.transform(X_val_title)\n",
    "del scaler1\n",
    "\n",
    "# Save to file\n",
    "np.savez(\"../Simple_995.npz\", \n",
    "         Y_train=Y_train,\n",
    "         Y_test=Y_test,\n",
    "         Y_val=Y_val,\n",
    "         X_train_content_BOW=X_train_content_BOW, \n",
    "         X_test_content_BOW=X_test_content_BOW, \n",
    "         X_val_content_BOW=X_val_content_BOW,\n",
    "         X_train_content_ONEHOT=X_train_content_ONEHOT, \n",
    "         X_test_content_ONEHOT=X_test_content_ONEHOT, \n",
    "         X_val_content_ONEHOT=X_val_content_ONEHOT,\n",
    "         X_train_title=X_train_title, \n",
    "         X_test_title=X_test_title, \n",
    "         X_val_title=X_val_title,\n",
    "         X_train_domain=X_train_domain, \n",
    "         X_test_domain=X_test_domain, \n",
    "         X_val_domain=X_val_domain)\n",
    "\n",
    "# Delete unnecessary variables\n",
    "del Y_train, Y_test, Y_val, X_train_content_BOW, X_test_content_BOW, X_val_content_BOW, X_train_content_ONEHOT, X_test_content_ONEHOT, X_val_content_ONEHOT, X_train_title, X_test_title, X_val_title, X_train_domain, X_test_domain, X_val_domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature representation for simple model with 995,000 and BBC articles\n",
    "\n",
    "The following code box creates Feature representations for (part 2, task 3).\n",
    "\n",
    "CountVectorizer(bag of words) encoding for 995,000_rows combined with scraped BBC articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for task 2.3\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#read content and types of 995,000_rows and BBC articles\n",
    "content_995_BBC = np.vstack((pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[5]).values,pd.read_csv(\"../BBC_articles_preprocessed.csv\",usecols=[2]).values))\n",
    "Y_995_BBC = np.vstack((pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[3]).values,pd.read_csv(\"../BBC_articles_preprocessed.csv\",usecols=[1]).values))\n",
    "#transform types into bool: 1-real, 0-fake\n",
    "Y_995_BBC = np.isin(Y_995_BBC,['reliable', 'political']).astype(int)\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = split_data(content_995_BBC,Y_995_BBC)\n",
    "del content_995_BBC,Y_995_BBC\n",
    "\n",
    "#bag of words:these parameters excludes features with frequency of under 100, and the words in stop_words.\n",
    "count_vectorizer = CountVectorizer(min_df=100,lowercase= False,stop_words=['NUM', 'URL', 'EMAIL', 'DATE'])\n",
    "# Fit and transform \n",
    "X_train = count_vectorizer.fit_transform(X_train.reshape(-1))\n",
    "X_test = count_vectorizer.transform(X_test.reshape(-1))\n",
    "X_val = count_vectorizer.transform(X_val.reshape(-1))\n",
    "\n",
    "#scale content features. this is recommended for BOW/countVectorizer features\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "#write to file\n",
    "np.savez(\"../Simple_995_BBC.npz\", X_train=X_train, X_test=X_test, X_val=X_val, Y_train=Y_train, Y_test=Y_test, Y_val=Y_val)\n",
    "del count_vectorizer, X_train, X_test, X_val, Y_train, Y_test, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature representations for complex model.\n",
    "\n",
    "The following code box creates Feature representations for (part 3).\n",
    "\n",
    "tf-idf encoding for 995,000_rows and BBC articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "#995,000_rows and BBC articles\n",
    "content = np.vstack((pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[5]).values,pd.read_csv(\"../BBC_articles_preprocessed.csv\",usecols=[2]).values))\n",
    "Y = np.vstack((pd.read_csv(\"../995,000_rows_preprocessed.csv\",usecols=[3]),pd.read_csv(\"../BBC_articles_preprocessed.csv\",usecols=[1]).values))\n",
    "#transform types into bool: 1-real, 0-fake\n",
    "Y = np.isin(Y,['reliable', 'political']).astype(int)\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = split_data(content,Y)\n",
    "del content, Y\n",
    "\n",
    "#bag of words:these parameters excludes features with frequency of under 100, and the words in stop_words.\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=100,lowercase= False,stop_words=['NUM', 'URL', 'EMAIL', 'DATE'])\n",
    "# Fit and transform \n",
    "X_train = tfidf_vectorizer.fit_transform(X_train.reshape(-1))\n",
    "X_test = tfidf_vectorizer.transform(X_test.reshape(-1))\n",
    "X_val = tfidf_vectorizer.transform(X_val.reshape(-1))\n",
    "np.savez(\"../advanced_features.npz\", X_train=X_train, X_test=X_test, X_val=X_val, Y_train=Y_train, Y_test=Y_test, Y_val=Y_val)\n",
    "del X_train, X_test, X_val, Y_train, Y_test, Y_val \n",
    "\n",
    "#LIAR dataset\n",
    "with zipfile.ZipFile(\"../liar_dataset.zip\",'r') as zip_file:\n",
    "    with zip_file.open(\"test.tsv\") as test_tsv:\n",
    "        test = pd.read_csv(test_tsv, sep='\\t')\n",
    "    with zip_file.open(\"valid.tsv\") as valid_tsv:\n",
    "        valid = pd.read_csv(valid_tsv, sep='\\t')\n",
    "\n",
    "#transform LIAR\n",
    "X_test = tfidf_vectorizer.transform(test.iloc[:, 2].values.reshape(-1))\n",
    "X_val= tfidf_vectorizer.transform(valid.iloc[:, 2].values.reshape(-1))\n",
    "Y_test= test.iloc[:,1].apply(lambda x: 1 if x in ['true','mostly-true'] else 0).values\n",
    "Y_val = valid.iloc[:,1].apply(lambda x: 1 if x in ['true','mostly-true'] else 0).values\n",
    "np.savez(\"../LIAR_features.npz\", X_test= X_test, X_val=X_val, Y_test=Y_test, Y_val=Y_val)\n",
    "del test, valid, X_test, X_val, tfidf_vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
