{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: advanced model\n",
    "\n",
    "Many different methods have been tried for the advanced model, here are the ones that yielded meaningful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "data = np.load(\"../advanced_features.npz\", allow_pickle=True)\n",
    "X_train = data['X_train'].item()\n",
    "X_val= data['X_val'].item()\n",
    "Y_train = data['Y_train'].ravel()\n",
    "Y_val = data['Y_val'].ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes with tf-idf:\n",
    "\n",
    "Using multinominal naive bayes (the variant that incorporates word frequency features) in conjunction with tf-idf encoding. Yields relatively poorer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83     43081\n",
      "           1       0.83      0.81      0.82     42014\n",
      "\n",
      "    accuracy                           0.83     85095\n",
      "   macro avg       0.83      0.83      0.83     85095\n",
      "weighted avg       0.83      0.83      0.83     85095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_val)\n",
    "print(classification_report(Y_val, Y_pred))\n",
    "\n",
    "with open(\"../advanced_model_NB.pkl\",'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "del model,Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### support vector machine with tf-idf:\n",
    "\n",
    "This is the complex model we have settled with, as it produces the best results out of all tested models. \n",
    "\n",
    "The linearSVC() has trouble converging when fitting the dataset. From experimenting with hyperparameters, I have found that beyond around 100 iterations, the benchmarks no longer improve, and are maxed out at around 88 for everything. The most iterations tried is 1000 iterations. \n",
    "\n",
    "There is another module from sklearn, SVC(), which offers support vector machines using nonlinear kernels, which might create better results. However, SVC()'s time complexity scales at least quadratically with number of samples. therefore, since our dataset has close to a million samples, we will be using LinearSVC instead. \n",
    "\n",
    "Attempting to run the normal SVC(), even with fractions of the 995,000 dataset has resulted in hours of compiling with no results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyu/PythonEnvironments/general/lib/python3.9/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with RBF Kernel Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88     43081\n",
      "           1       0.89      0.87      0.88     42014\n",
      "\n",
      "    accuracy                           0.88     85095\n",
      "   macro avg       0.88      0.88      0.88     85095\n",
      "weighted avg       0.88      0.88      0.88     85095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "model1 = LinearSVC(dual=\"auto\",tol=1e-3, C=0.5,max_iter=300)\n",
    "model1.fit(X_train,Y_train)\n",
    "Y_pred = model1.predict(X_val)\n",
    "\n",
    "print(\"SVM with RBF Kernel Classification Report:\")\n",
    "print(classification_report(Y_val, Y_pred))\n",
    "\n",
    "with open(\"../advanced_model_SVC.pkl\",'wb') as f:\n",
    "    pickle.dump(model1,f)\n",
    "\n",
    "del Y_pred, model1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
