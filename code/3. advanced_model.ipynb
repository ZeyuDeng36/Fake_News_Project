{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: advanced model\n",
    "\n",
    "Many different methods have been tried for the advanced model, here are the ones that yielded meaningful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "data = np.load(\"../advanced_features.npz\", allow_pickle=True)\n",
    "X_train = data['X_train'].item()\n",
    "X_test= data['X_test'].item()\n",
    "Y_train = data['Y_train'].ravel()\n",
    "Y_test = data['Y_test'].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with tf-idf encoding:\n",
    "\n",
    "This model yields good results, thanks to the tf-idf encoding. however, it still uses logistic regression, which is a \"simple\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyu/PythonEnvironments/general/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88     43036\n",
      "           1       0.88      0.86      0.87     42060\n",
      "\n",
      "    accuracy                           0.87     85096\n",
      "   macro avg       0.87      0.87      0.87     85096\n",
      "weighted avg       0.87      0.87      0.87     85096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = LogisticRegression(tol=1e-3)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "del model,Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes with tf-idf:\n",
    "\n",
    "Using multinominal naive bayes (the variant that incorporates word frequency features) in conjunction with tf-idf encoding. Yields relatively poorer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82     43036\n",
      "           1       0.82      0.80      0.81     42060\n",
      "\n",
      "    accuracy                           0.82     85096\n",
      "   macro avg       0.82      0.82      0.82     85096\n",
      "weighted avg       0.82      0.82      0.82     85096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "del model,Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### support vector machine with tf-idf:\n",
    "\n",
    "This is the complex model we have settled with, as it produces the best results out of all tested models. \n",
    "\n",
    "The linearSVC() has trouble converging when fitting the dataset. From experimenting with hyperparameters, I have found that beyond around 100 iterations, the benchmarks no longer improve, and are maxed out at around 88 for everything. The most iterations tried is 1000 iterations. \n",
    "\n",
    "There is another module from sklearn, SVC(), which offers support vector machines using nonlinear kernels, which might create better results. However, SVC()'s time complexity scales at least quadratically with number of samples. therefore, since our dataset has close to a million samples, we will be using LinearSVC instead. \n",
    "\n",
    "Attempting to run the normal SVC(), even with fractions of the 995,000 dataset has resulted in hours of compiling with no results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with RBF Kernel Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88     43036\n",
      "           1       0.88      0.87      0.88     42060\n",
      "\n",
      "    accuracy                           0.88     85096\n",
      "   macro avg       0.88      0.88      0.88     85096\n",
      "weighted avg       0.88      0.88      0.88     85096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyu/PythonEnvironments/general/lib/python3.9/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "model1 = LinearSVC(dual=\"auto\",max_iter=1000, C=0.5)\n",
    "model1.fit(X_train,Y_train)\n",
    "Y_pred = model1.predict(X_test)\n",
    "\n",
    "print(\"SVM with RBF Kernel Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "with open(\"../advanced_model.pkl\",'wb') as f:\n",
    "    pickle.dump(model1,f)\n",
    "\n",
    "del Y_pred, model1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
